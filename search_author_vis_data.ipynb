{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The histogram (x-axis) consists of binned readability score, (y-axis) counts of papers that occupy that readability score. \n",
    "\n",
    "The histogram is initially populated exclusively by the ART corpus, but the idea was every time a new author got scraped from scholar, it would be added in, such that with each persons new search our big picture of science readability would be better informed.\n",
    "\n",
    "Three dots pertaining to the authors easiest read, hardest read, and mean read where added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20344\r\n",
      "-rw-r--r--@  1 rjjarvis  staff       90 Apr 16 21:54 __init__.py\r\n",
      "-rw-r--r--@  1 rjjarvis  staff     2146 Apr 16 21:54 analysis.py\r\n",
      "-rw-r--r--@  1 rjjarvis  staff     5332 Apr 16 21:54 authors.py\r\n",
      "-rw-r--r--@  1 rjjarvis  staff     1226 Apr 16 21:54 enter_author_name.py\r\n",
      "-rw-r--r--@  1 rjjarvis  staff    14106 Apr 16 21:54 for_joss_competion.py\r\n",
      "drwxr-xr-x@  7 rjjarvis  staff      224 Apr 16 22:12 \u001b[34mscholar_scrape\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 rjjarvis  staff    14158 Apr 17 15:34 utils.py\r\n",
      "-rw-r--r--   1 rjjarvis  staff     4036 Apr 17 18:50 CONTRIBUTING.md\r\n",
      "-rw-r--r--@  1 rjjarvis  staff     1236 Apr 17 18:50 license.md\r\n",
      "-rw-r--r--@  1 rjjarvis  staff     6340 Apr 17 21:00 plotting_author_versus_distribution.py\r\n",
      "-rw-r--r--@  1 rjjarvis  staff   183730 Apr 17 21:34 original_distri.ipynb\r\n",
      "-rw-r--r--@  1 rjjarvis  staff    12500 Apr 17 21:34 original_distri.py\r\n",
      "drwxr-xr-x   4 rjjarvis  staff      128 Apr 26 11:57 \u001b[34mdata\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 rjjarvis  staff    11344 Apr 26 11:59 README.md\r\n",
      "-rw-r--r--   1 rjjarvis  staff      681 Apr 26 11:59 gecko_install.sh\r\n",
      "-rw-r--r--   1 rjjarvis  staff     5073 Apr 26 11:59 get_bmark_corpus.py\r\n",
      "-rw-r--r--   1 rjjarvis  staff      978 Apr 26 11:59 install.sh\r\n",
      "-rw-r--r--   1 rjjarvis  staff    50150 Apr 26 11:59 scholar.py\r\n",
      "-rw-r--r--   1 rjjarvis  staff    13314 Apr 26 11:59 scrape.py\r\n",
      "-rw-r--r--   1 rjjarvis  staff     5041 Apr 26 11:59 crawl.py\r\n",
      "drwxr-xr-x   6 rjjarvis  staff      192 Apr 26 12:00 \u001b[34mold\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 rjjarvis  staff     6376 Apr 26 12:00 online_app_backend.py.orig\r\n",
      "-rw-r--r--   1 rjjarvis  staff     6031 Apr 26 12:01 online_app_backend.py\r\n",
      "-rw-r--r--   1 rjjarvis  staff      112 Apr 26 12:03 requirements.txt\r\n",
      "-rw-r--r--   1 rjjarvis  staff     6921 Apr 26 14:20 paper.md\r\n",
      "-rw-r--r--   1 rjjarvis  staff        0 Jun 18 17:32 more_authors_results.p?dl=0\r\n",
      "-rw-r--r--   1 rjjarvis  staff    68232 Jun 18 17:32 benchmarks.p\r\n",
      "-rw-r--r--   1 rjjarvis  staff    35381 Jun 18 17:49 geckodriver.log\r\n",
      "-rw-r--r--   1 rjjarvis  staff     5165 Jun 18 17:49 _author_specific.p\r\n",
      "-rw-r--r--   1 rjjarvis  staff  9822110 Jun 18 17:49 traingDats.p\r\n",
      "drwxr-xr-x  10 rjjarvis  staff      320 Jun 18 17:49 \u001b[34m__pycache__\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 rjjarvis  staff    58107 Jun 18 17:51 search_author_vis_data.ipynb\r\n",
      "-rw-r--r--@  1 rjjarvis  staff     9629 Jun 18 17:52 t_analysis.py\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rjjarvis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rjjarvis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "mv: traingDats.p?dl=0: No such file or directory\n",
      "mv: benchmarks.p?dl=0: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#import scholar\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "try:\n",
    "    !mv traingDats.p?dl=0 traingDats.p\n",
    "    !mv benchmarks.p?dl=0 benchmarks.p\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from online_app_backend import call_from_front_end\n",
    "from online_app_backend import ar_manipulation\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "#from sklearn import datasets\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "st.write(\"\"\"\n",
    "  Please Enter the scholar Author you would like to search for\n",
    "\"\"\")\n",
    "\n",
    "st.sidebar.header('User Input Parameters')\n",
    "\n",
    "def user_input_features():\n",
    "    sepal_length = st.sidebar.slider('Sepal length', 4.3, 7.9, 5.4)\n",
    "    sepal_width = st.sidebar.slider('Sepal width', 2.0, 4.4, 3.4)\n",
    "    petal_length = st.sidebar.slider('Petal length', 1.0, 6.9, 1.3)\n",
    "    petal_width = st.sidebar.slider('Petal width', 0.1, 2.5, 0.2)\n",
    "    data = {'sepal_length': sepal_length,\n",
    "            'sepal_width': sepal_width,\n",
    "            'petal_length': petal_length,\n",
    "            'petal_width': petal_width}\n",
    "    features = pd.DataFrame(data, index=[0])\n",
    "    return features\n",
    "\n",
    "df = user_input_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0e5f0e67e342bbac0d3bcb32931243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Query:', placeholder='Enter scholar Author')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C3&q=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rjjarvis/git/wcomplexity_dash/plotting_author_versus_distribution.py:6: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 597, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1199, in inner\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1113, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 315, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 315, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 315, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2666, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/events.py\", line 88, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/pylab/backend_inline.py\", line 168, in configure_once\n",
      "    activate_matplotlib(backend)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/anaconda3/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  mpl.use(\"Agg\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'more_authors_results.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ce0a7dbd195b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_from_front_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/wcomplexity_dash/online_app_backend.py\u001b[0m in \u001b[0;36mcall_from_front_end\u001b[0;34m(NAME, tour, NAME1, verbose)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'traingDats.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingDats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mplotting_author_versus_distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/wcomplexity_dash/plotting_author_versus_distribution.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mbmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'benchmarks.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mNAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'more_authors_results.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#print(ar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbmark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'more_authors_results.p'"
     ]
    }
   ],
   "source": [
    "from online_app_backend import call_from_front_end\n",
    "from online_app_backend import ar_manipulation\n",
    "import ipywidgets as widgets\n",
    "#from IPython.html import widgets # Widget definitions\n",
    "year_output = widgets.Text()\n",
    "\n",
    "# Create text widget for input\n",
    "year_input = widgets.Text(\n",
    "    placeholder=\"Enter scholar Author\",\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    "    )\n",
    "\n",
    "# Define function to bind value of the input to the output variable\n",
    "def bind_input_to_output(sender):\n",
    "    year_output.value = year_input.value\n",
    "    \n",
    "    #return results\n",
    "# Tell the text input widget to call bind_input_to_output() on submit\n",
    "\n",
    "year_input.on_submit(bind_input_to_output)\n",
    "display(year_input)\n",
    "\n",
    "results = call_from_front_end(year_output.value)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1,0],[0,1])\n",
    "plt.show()\n",
    "from online_app_backend import call_from_front_end\n",
    "from online_app_backend import ar_manipulation\n",
    "import numpy as np\n",
    "import pickle\n",
    "from plotting_author_versus_distribution import PlotUtils\n",
    "\n",
    "NAME = \"S Phatak\"\n",
    "try:\n",
    "    ar = pickle.load(open('more_authors_results.p','rb'))\n",
    "except:\n",
    "    results = call_from_front_end(NAME)\n",
    "    ar = results[-1] \n",
    "    (ar, trainingDats) = ar_manipulation(ar)\n",
    "    with open(str('more_authors_results.p'),'wb') as f:\n",
    "        pickle.dump(ar,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "if os.path.exists('traingDats.p?dl=0'):\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    !wget https://www.dropbox.com/s/3h12l5y2pn49c80/traingDats.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/crarli3772rf3lj/more_authors_results.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/x66zf52himmp5ox/benchmarks.p?dl=0\n",
    "trainingDats = pickle.load(open('traingDats.p?dl=0','rb'))\n",
    "standard_sci = [ t['standard'] for t in trainingDats ]\n",
    "\n",
    "group_labels = ['Biochemistry Documents']#, 'Group 2', 'Group 3']\n",
    "colors = ['#393E46']#, '#2BCDC1', '#F66095']\n",
    "\n",
    "fig = ff.create_distplot([standard_sci], group_labels, colors=colors,\n",
    "                         bin_size=[0.3, 0.2, 0.1], show_curve=True)\n",
    "\n",
    "fig.update(layout_title_text='Art Corpus')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ = np.min([a['standard'] for a in ar])\n",
    "plts = PlotUtils(NAME,ar)\n",
    "plt,df0,df1,df2,df3,values,fig,ax = plts.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bench = df0['benchmarks'].values\n",
    "#bench = df0['benchmarks'].values\n",
    "\n",
    "stats = df1['mean, min, maximum'].values\n",
    "this_doc = df2['Standard Reading Level'].values\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDats = pickle.load(open('traingDats.p?dl=0','rb'))\n",
    "standard_sci = [ t['standard'] for t in trainingDats ]\n",
    "['Benchmarks', 'stats']\n",
    "group_labels = ['Biochemistry Documents','this doc']\n",
    "#colors = ['#393E46', '#2BCDC1', '#F66095','#G56095' ]\n",
    "fig = ff.create_distplot([standard_sci,this_doc], group_labels, \n",
    "                         bin_size=[0.5, 0.15], show_curve=True)\n",
    "\n",
    "fig.update(layout_title_text='Art Corpus')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(df, x=\"readabilty\", y=\"document\", color=\"document\",\n",
    "                   marginal=\"violin\",\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'readabilty':standard_sci,'document':list(range(0,len(standard_sci)))})\n",
    "df2 = pd.DataFrame({'this_doc':this_doc})\n",
    "import plotly.express as px\n",
    "\n",
    "#df = px.data.tips()\n",
    "fig = px.violin(df, y=\"readabilty\", x=\"document\", box=True,\n",
    "          hover_data=df.columns)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be easy to hack this code to run off a local machine, using sudo.\n",
    "Set up the Environment. This is now done in requirements, and the postBuild script.\n",
    "```python\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "\n",
    "if os.path.exists('traingDats.p?dl=0'):\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    !wget https://www.dropbox.com/s/3h12l5y2pn49c80/traingDats.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/crarli3772rf3lj/more_authors_results.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/x66zf52himmp5ox/benchmarks.p?dl=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import copy\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#import glob\n",
    "#files = glob.glob(\"*.p\")\n",
    "#discontents = pickle.load(open(\"_author_specificS S Phatak.p\",\"rb\"))\n",
    "#type(discontents[0])\n",
    "#df = discontents[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('scraped_new.p?dl=0','rb') as f:\n",
    "        texts = pickle.load(f) \n",
    "except:\n",
    "    !wget https://www.dropbox.com/s/1kc7alp79h701hx/scraped_new.p?dl=0\n",
    "    with open('scraped_new.p?dl=0','rb') as f:\n",
    "        texts = pickle.load(f) \n",
    "\n",
    "queries = set([t['query'] for t in texts ])\n",
    "temp = [t for t in texts if 'standard' in t.keys() and 'wikipedia' in t['link']]\n",
    "science = ['cancer','Vaccines','evolution','climate change','Transgenic','photosysnthesis','evolution','GMO']\n",
    "res = [t['standard'] for t in temp if t['query'] in science]\n",
    "mwp = np.mean(res)  \n",
    "abstract_wiki = {'standard':mwp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "fig = px.histogram(df, x=\"total_bill\", y=\"tip\", color=\"sex\",\n",
    "                   marginal=\"box\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if os.path.exists('traingDats.p?dl=0'):\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    !wget https://www.dropbox.com/s/3h12l5y2pn49c80/traingDats.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/crarli3772rf3lj/more_authors_results.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/x66zf52himmp5ox/benchmarks.p?dl=0\n",
    "with open('traingDats.p?dl=0','rb') as f:\n",
    "        trainingDats = pickle.load(f) \n",
    "\n",
    "bmark = pickle.load(open('benchmarks.p?dl=0','rb'))\n",
    "\n",
    "ar = discontents[2]\n",
    "np.mean(df['standard'])\n",
    "NAME = \"Sayali S. Phatak\"\n",
    "trainingDats.extend(bmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([b['standard'] for b in bmark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_heights(stats_items,histogram_content,x_sub_set):\n",
    "    vertical_postions_indexs = []\n",
    "    for i in stats_items:\n",
    "        vertical_postions_indexs.append(find_nearest(histogram_content, i))\n",
    "    bin_width_offset = (xys[1][0] - xys[0][0])/2.0\n",
    "    x_sub_set = [ i+bin_width_offset for i in x_sub_set ]\n",
    "\n",
    "\n",
    "    heights = []\n",
    "    for i in vertical_postions_indexs:\n",
    "        heights.append(xys[i][1])\n",
    "    return heights, bin_width_offset\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def snap_to_grid(author_stats,bin_centers):\n",
    "    author_stats_grid = []\n",
    "    for as_ in author_stats:\n",
    "        as_ = find_nearest(bin_centers,as_)\n",
    "        author_stats_grid.append(bin_centers[as_])\n",
    "    return author_stats_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ART = np.max([ t['standard'] for t in trainingDats ])\n",
    "publication = [ t['publication'] for t in trainingDats if t['standard'] == max_ART ]\n",
    "keys = [ t.keys() for t in trainingDats if t['standard'] == max_ART ]\n",
    "\n",
    "fname = [ t['file_name'] for t in trainingDats if t['standard'] == max_ART ]\n",
    "bmark_max_art = {'standard':max_ART}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "standard_sci = [ t['standard'] for t in trainingDats ]\n",
    "ar = [ t for t in ar if type(t) is type({})]\n",
    "ar = [ t for t in ar if 'standard' in t.keys()]\n",
    "xys = [ (h.get_x(),h.get_height()) for h in sns.distplot(standard_sci).patches ]\n",
    "\n",
    "x_grid = [ h.get_x() for h in sns.distplot(standard_sci).patches ]\n",
    "offset = float((x_grid[1] - x_grid[0])/2.0)\n",
    "bin_centers = [gr+offset for gr in x_grid]\n",
    "# this plot not used yet.\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8), dpi=80)\n",
    "ax1 = fig.add_subplot(111)#)\n",
    "mean_ = np.mean([a['standard'] for a in ar])\n",
    "min_ = np.min([a['standard'] for a in ar])\n",
    "max_ = np.max([a['standard'] for a in ar])\n",
    "std_ = np.std([a['standard'] for a in ar])\n",
    "stats_items = [mean_,min_,max_]\n",
    "\n",
    "g = sns.distplot(standard_sci, label=\"Readability Index\")\n",
    "\n",
    "\n",
    "histogram_content = [x[0] for x in xys]\n",
    "height_content = np.array([x[1] for x in xys])\n",
    "\n",
    "hc = np.array(histogram_content)\n",
    "\n",
    "x_sub_set=histogram_content\n",
    "\n",
    "\n",
    "other_name=str('Phytochromobilin C15-Z,syn - C15-E,anti isomerization: concerted or stepwise?')\n",
    "worst_height,_ = get_heights([max_ART],hc,x_sub_set)\n",
    "mwp_height,_ = get_heights([mwp],hc,x_sub_set)\n",
    "\n",
    "#bmark_max_art\n",
    "worst_height = worst_height[0]\n",
    "#bmark_stats_items_grid = snap_to_grid(bmark_stats_items,bin_centers)\n",
    "\n",
    "#worst_distamnce = snap_to_grid(max_ART,bin_centers)\n",
    "worst_distance = snap_to_grid([max_ART],bin_centers)\n",
    "mwp_distance = snap_to_grid([mwp],bin_centers)\n",
    "x,y,z = (mwp_distance[0],mwp_height[0],str('mean wikipedia'))\n",
    "\n",
    "#print(bmark)\n",
    "bmark_stats_items = list(set([ b['standard'] for b in bmark ]))\n",
    "bmark_stats_items.append(x)\n",
    "#bmark_stats_items.append(max_ART)\n",
    "bmark_heights, _ = get_heights(bmark_stats_items,histogram_content,x_sub_set)\n",
    "heights, bwo = get_heights(stats_items,histogram_content,x_sub_set)\n",
    "#bmark_heights.append(worst_height)\n",
    "bmark_stats_items = [i+bwo for i in bmark_stats_items]\n",
    "mean_a = mean_\n",
    "min_a = min_ \n",
    "max_a = max_ \n",
    "xticks = list(range(0,45,5))\n",
    "\n",
    "#print(xticks)\n",
    "bmark_stats_items\n",
    "box_content = [a['standard'] for a in ar]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bmark_stats_items_grid = snap_to_grid(bmark_stats_items,bin_centers)\n",
    "author_stats =[i for i in [mean_,min_,max_]]\n",
    "author_stats_grid = snap_to_grid(author_stats,bin_centers)\n",
    "mean_a_grid = snap_to_grid([mean_a],bin_centers)\n",
    "x_sub_set_grid = snap_to_grid(x_sub_set,bin_centers)\n",
    "\n",
    "print(bmark_stats_items_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ \"Readibility of Science Declining Over Time\", \"Post Modern Essay Generator\",\"upgoer 5\",\"Science of Writing\",\"Mean Wikipedia\"]#\"Mean Wikipedia\"]#,other_name]# \"wikipedia science\"]\n",
    "bmark_stats_items_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalibrate_heights,b = get_heights(author_stats_grid,hc,x_sub_set)\n",
    "\n",
    "heights[0] = np.max(recalibrate_heights)\n",
    "heights[2] = recalibrate_heights[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(height_content)\n",
    "heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmark_heights\n",
    "print(len(bmark_heights))\n",
    "print(len(bin_centers))\n",
    "print(len(bmark_stats_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bmark_stats_items_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = np.array(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmark_stats_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories\n",
    "#categories.insert(3,'Mean Wikipedia Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xinterval\n",
    "x1,y1,z1 = (mwp_distance[0],mwp_height[0],str('mean wikipedia'))\n",
    "x1\n",
    "#bmark_heights[3]=y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(bmark_stats_items_grid)\n",
    "import copy\n",
    "#del bmark_stats_items_grid[-2]\n",
    "#del bmark_stats_items_grid[-1]\n",
    "xinterval1 = copy.copy(bmark_stats_items_grid)\n",
    "#xinterval1.insert(3,x1)\n",
    "#xinterval1\n",
    "#del bmark_heights[-1]\n",
    "bmark_heights\n",
    "print(len(bmark_heights))\n",
    "print(len(bmark_stats_items_grid))\n",
    "\n",
    "benchmarks = pd.DataFrame({\n",
    "'benchmarks': bmark_stats_items_grid,\n",
    "    'CDF': bmark_heights\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 10),nrows=2, ncols=1, sharex=True, dpi=100)\n",
    "\n",
    "\n",
    "g = sns.distplot(standard_sci, label=\"Readability Index\")\n",
    "\n",
    "\n",
    "if str('data0') not in locals():\n",
    "    data0 = pd.DataFrame({\n",
    "    'mean, min, maximum': author_stats_grid,\n",
    "        'CDF': heights\n",
    "        })\n",
    "\n",
    "\n",
    "    data2 = pd.DataFrame({\n",
    "    'Standard Reading Level': mean_a_grid,\n",
    "        'CDF': np.max(height_content)\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "legend_properties = {'weight':'bold','size':8}\n",
    "ax = sns.regplot(data=benchmarks, x=\"benchmarks\", y=\"CDF\", fit_reg=False, marker=\"o\", color=\"green\")\n",
    "ax = sns.regplot(data=data2, x=\"Standard Reading Level\", y=\"CDF\", fit_reg=False, marker=\"o\", color=\"red\")\n",
    "legendMain=ax.legend(labels=[str(\"std deviation\")], prop=legend_properties,loc='upper right')\n",
    "legendSide0=ax.legend(labels=[NAME],prop=legend_properties,loc='center right')\n",
    "legendSide1=ax.legend(labels=[str('Number of Documents: '+str(len(ar)))],prop=legend_properties,loc='upper left')\n",
    "legendMain=ax.legend(labels=[str(\"Google scholar author relative to ART Corpus distribution. Total docs: \")+str(len(trainingDats))], prop=legend_properties,loc='upper left')\n",
    "#\n",
    "print(categories)\n",
    "x,y,z = (worst_distance[0],worst_height,other_name)\n",
    "data3 = pd.DataFrame({\n",
    "'Standard Reading Level': [x1],\n",
    "    'CDF': [y1]\n",
    "    })\n",
    "ax = sns.regplot(data=data3, x='Standard Reading Level', y=\"CDF\", fit_reg=False, marker=\"o\", color=\"green\")\n",
    "\n",
    "\n",
    "axes[1] = ax = sns.regplot(data=benchmarks, x=\"benchmarks\", y=\"CDF\", fit_reg=False, marker=\"o\", color=\"green\")\n",
    "\n",
    "ax2 = plt.twiny()\n",
    "xticks = list(range(0,45,5))\n",
    "ax2.set_xticks(xticks)\n",
    "\n",
    "axes[1].set_xticks(xinterval1)\n",
    "axes[1].set_xticklabels(categories, minor=False, rotation=90)\n",
    "\n",
    "axes[1].axvline(np.mean(standard_sci), color='red', alpha=.7, linewidth=1.5)\n",
    "axes[1].set_ylabel('Probability of Document Reading Level')\n",
    "axes[1].set_xlabel('Reading Grade Level')\n",
    "bp_dict = axes[0].boxplot(box_content, 0, 'gD', vert=False)\n",
    "\n",
    "\n",
    "for line in bp_dict['medians']:\n",
    "    x, y = line.get_xydata()[1] # top of median line\n",
    "\n",
    "for line in bp_dict['boxes']:\n",
    "    x0, y = line.get_xydata()[0] # bottom of left line\n",
    "    axes[0].text(x0,y, str(NAME)+' Q1 ',horizontalalignment='center',verticalalignment='top',rotation=90)\n",
    "\n",
    "    x1, y = line.get_xydata()[3] # bottom of right line\n",
    "    axes[0]. text(x1,y, str(NAME)+' Q3 ',horizontalalignment='center',verticalalignment='top',rotation=90)\n",
    "\n",
    "    axes[0]. text(np.abs(x1+x0)/2,y, str(NAME)+' $\\mu$ ',horizontalalignment='center',verticalalignment='top',rotation=90)\n",
    "    x2, y = line.get_xydata()[1] # bottom of right line\n",
    "axes[0].axvline(np.mean(standard_sci), color='red', alpha=.7, linewidth=1.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmark_stats_items_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

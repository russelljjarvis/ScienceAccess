{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The histogram (x-axis) consists of binned readability score, (y-axis) counts of papers that occupy that readability score. \n",
    "\n",
    "The histogram is initially populated exclusively by the ART corpus, but the idea was every time a new author got scraped from scholar, it would be added in, such that with each persons new search our big picture of science readability would be better informed.\n",
    "\n",
    "Three dots pertaining to the authors easiest read, hardest read, and mean read where added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import scholar\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f39b8d7fcec4f88919c2b40e18cf32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Query:', placeholder='Enter scholar Author')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.com/scholar?hl=en&as_sdt=0%2C3&q=\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/user/nltk_data'\n    - '/home/user/anaconda3/nltk_data'\n    - '/home/user/anaconda3/share/nltk_data'\n    - '/home/user/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/user/nltk_data'\n    - '/home/user/anaconda3/nltk_data'\n    - '/home/user/anaconda3/share/nltk_data'\n    - '/home/user/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce0a7dbd195b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_from_front_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ScienceAccess/online_app_backend.py\u001b[0m in \u001b[0;36mcall_from_front_end\u001b[0;34m(NAME, tour, NAME1, verbose)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtour\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mscholar_link\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://scholar.google.com/scholar?hl=en&as_sdt=0%2C3&q='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0menter_name_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscholar_link\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_author_specific'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscholar_link\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ScienceAccess/online_app_backend.py\u001b[0m in \u001b[0;36menter_name_here\u001b[0;34m(scholar_page, name)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0menter_name_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscholar_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_web_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscholar_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;31m#author_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     '''\n",
      "\u001b[0;32m~/git/ScienceAccess/online_app_backend.py\u001b[0m in \u001b[0;36mupdate_web_form\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m#data = author_results = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mauthor_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_url_from_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m#data[name] = author_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ScienceAccess/online_app_backend.py\u001b[0m in \u001b[0;36mtake_url_from_gui\u001b[0;34m(author_link_scholar_link_list)\u001b[0m\n\u001b[1;32m     61\u001b[0m            \u001b[0mfollow_more_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_pubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfollow_more_links\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                \u001b[0murlDat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;31m#print(urlDat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ScienceAccess/get_bmark_corpus.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpdf_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mbuffered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_pdf_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0murlDat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_proc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murlDat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0murlDat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/ScienceAccess/t_analysis.py\u001b[0m in \u001b[0;36mtext_proc\u001b[0;34m(corpus, urlDat, WORD_LIM)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;31m#We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/user/nltk_data'\n    - '/home/user/anaconda3/nltk_data'\n    - '/home/user/anaconda3/share/nltk_data'\n    - '/home/user/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from online_app_backend import call_from_front_end\n",
    "from online_app_backend import ar_manipulation\n",
    "import ipywidgets as widgets\n",
    "#from IPython.html import widgets # Widget definitions\n",
    "year_output = widgets.Text()\n",
    "\n",
    "# Create text widget for input\n",
    "year_input = widgets.Text(\n",
    "    placeholder=\"Enter scholar Author\",\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    "    )\n",
    "\n",
    "# Define function to bind value of the input to the output variable\n",
    "def bind_input_to_output(sender):\n",
    "    year_output.value = year_input.value\n",
    "    \n",
    "    #return results\n",
    "# Tell the text input widget to call bind_input_to_output() on submit\n",
    "\n",
    "year_input.on_submit(bind_input_to_output)\n",
    "display(year_input)\n",
    "\n",
    "results = call_from_front_end(year_output.value)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1,0],[0,1])\n",
    "plt.show()\n",
    "from online_app_backend import call_from_front_end\n",
    "from online_app_backend import ar_manipulation\n",
    "import numpy as np\n",
    "import pickle\n",
    "from plotting_author_versus_distribution import PlotUtils\n",
    "\n",
    "NAME = \"S Phatak\"\n",
    "try:\n",
    "    ar = pickle.load(open('more_authors_results.p','rb'))\n",
    "except:\n",
    "    results = call_from_front_end(NAME)\n",
    "    ar = results[-1] \n",
    "    (ar, trainingDats) = ar_manipulation(ar)\n",
    "    with open(str('more_authors_results.p'),'wb') as f:\n",
    "        pickle.dump(ar,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "if os.path.exists('traingDats.p?dl=0'):\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    !wget https://www.dropbox.com/s/3h12l5y2pn49c80/traingDats.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/crarli3772rf3lj/more_authors_results.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/x66zf52himmp5ox/benchmarks.p?dl=0\n",
    "trainingDats = pickle.load(open('traingDats.p?dl=0','rb'))\n",
    "standard_sci = [ t['standard'] for t in trainingDats ]\n",
    "\n",
    "group_labels = ['Biochemistry Documents']#, 'Group 2', 'Group 3']\n",
    "colors = ['#393E46']#, '#2BCDC1', '#F66095']\n",
    "\n",
    "fig = ff.create_distplot([standard_sci], group_labelad aas, colors=colors,\n",
    "                         bin_size=[0.3, 0.2, 0.1], show_curve=True)\n",
    "\n",
    "fig.update(layout_title_text='Art Corpus')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ = np.min([a['standard'] for a in ar])\n",
    "plts = PlotUtils(NAME,ar)\n",
    "plt,df0,df1,df2,df3,values,fig,ax = plts.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bench = df0['benchmarks'].values\n",
    "#bench = df0['benchmarks'].values\n",
    "\n",
    "stats = df1['mean, min, maximum'].values\n",
    "this_doc = df2['Standard Reading Level'].values\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDats = pickle.load(open('traingDats.p?dl=0','rb'))\n",
    "standard_sci = [ t['standard'] for t in trainingDats ]\n",
    "['Benchmarks', 'stats']\n",
    "group_labels = ['Biochemistry Documents','this doc']\n",
    "#colors = ['#393E46', '#2BCDC1', '#F66095','#G56095' ]\n",
    "fig = ff.create_distplot([standard_sci,this_doc], group_labels, \n",
    "                         bin_size=[0.5, 0.15], show_curve=True)\n",
    "\n",
    "fig.update(layout_title_text='Art Corpus')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(df, x=\"readabilty\", y=\"document\", color=\"document\",\n",
    "                   marginal=\"violin\",\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'readabilty':standard_sci,'document':list(range(0,len(standard_sci)))})\n",
    "df2 = pd.DataFrame({'this_doc':this_doc})\n",
    "import plotly.express as px\n",
    "\n",
    "#df = px.data.tips()\n",
    "fig = px.violin(df, y=\"readabilty\", x=\"document\", box=True,\n",
    "          hover_data=df.columns)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be easy to hack this code to run off a local machine, using sudo.\n",
    "Set up the Environment. This is now done in requirements, and the postBuild script.\n",
    "```python\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "\n",
    "if os.path.exists('traingDats.p?dl=0'):\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    !wget https://www.dropbox.com/s/3h12l5y2pn49c80/traingDats.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/crarli3772rf3lj/more_authors_results.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/x66zf52himmp5ox/benchmarks.p?dl=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import copy\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#import glob\n",
    "#files = glob.glob(\"*.p\")\n",
    "#discontents = pickle.load(open(\"_author_specificS S Phatak.p\",\"rb\"))\n",
    "#type(discontents[0])\n",
    "#df = discontents[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('scraped_new.p?dl=0','rb') as f:\n",
    "        texts = pickle.load(f) \n",
    "except:\n",
    "    !wget https://www.dropbox.com/s/1kc7alp79h701hx/scraped_new.p?dl=0\n",
    "    with open('scraped_new.p?dl=0','rb') as f:\n",
    "        texts = pickle.load(f) \n",
    "\n",
    "queries = set([t['query'] for t in texts ])\n",
    "temp = [t for t in texts if 'standard' in t.keys() and 'wikipedia' in t['link']]\n",
    "science = ['cancer','Vaccines','evolution','climate change','Transgenic','photosysnthesis','evolution','GMO']\n",
    "res = [t['standard'] for t in temp if t['query'] in science]\n",
    "mwp = np.mean(res)  \n",
    "abstract_wiki = {'standard':mwp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "fig = px.histogram(df, x=\"total_bill\", y=\"tip\", color=\"sex\",\n",
    "                   marginal=\"box\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if os.path.exists('traingDats.p?dl=0'):\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    !wget https://www.dropbox.com/s/3h12l5y2pn49c80/traingDats.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/crarli3772rf3lj/more_authors_results.p?dl=0\n",
    "    !wget https://www.dropbox.com/s/x66zf52himmp5ox/benchmarks.p?dl=0\n",
    "with open('traingDats.p?dl=0','rb') as f:\n",
    "        trainingDats = pickle.load(f) \n",
    "\n",
    "bmark = pickle.load(open('benchmarks.p?dl=0','rb'))\n",
    "\n",
    "ar = discontents[2]\n",
    "np.mean(df['standard'])\n",
    "NAME = \"Sayali S. Phatak\"\n",
    "trainingDats.extend(bmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([b['standard'] for b in bmark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_heights(stats_items,histogram_content,x_sub_set):\n",
    "    vertical_postions_indexs = []\n",
    "    for i in stats_items:\n",
    "        vertical_postions_indexs.append(find_nearest(histogram_content, i))\n",
    "    bin_width_offset = (xys[1][0] - xys[0][0])/2.0\n",
    "    x_sub_set = [ i+bin_width_offset for i in x_sub_set ]\n",
    "\n",
    "\n",
    "    heights = []\n",
    "    for i in vertical_postions_indexs:\n",
    "        heights.append(xys[i][1])\n",
    "    return heights, bin_width_offset\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def snap_to_grid(author_stats,bin_centers):\n",
    "    author_stats_grid = []\n",
    "    for as_ in author_stats:\n",
    "        as_ = find_nearest(bin_centers,as_)\n",
    "        author_stats_grid.append(bin_centers[as_])\n",
    "    return author_stats_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ART = np.max([ t['standard'] for t in trainingDats ])\n",
    "publication = [ t['publication'] for t in trainingDats if t['standard'] == max_ART ]\n",
    "keys = [ t.keys() for t in trainingDats if t['standard'] == max_ART ]\n",
    "\n",
    "fname = [ t['file_name'] for t in trainingDats if t['standard'] == max_ART ]\n",
    "bmark_max_art = {'standard':max_ART}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "standard_sci = [ t['standard'] for t in trainingDats ]\n",
    "ar = [ t for t in ar if type(t) is type({})]\n",
    "ar = [ t for t in ar if 'standard' in t.keys()]\n",
    "xys = [ (h.get_x(),h.get_height()) for h in sns.distplot(standard_sci).patches ]\n",
    "\n",
    "x_grid = [ h.get_x() for h in sns.distplot(standard_sci).patches ]\n",
    "offset = float((x_grid[1] - x_grid[0])/2.0)\n",
    "bin_centers = [gr+offset for gr in x_grid]\n",
    "# this plot not used yet.\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8), dpi=80)\n",
    "ax1 = fig.add_subplot(111)#)\n",
    "mean_ = np.mean([a['standard'] for a in ar])\n",
    "min_ = np.min([a['standard'] for a in ar])\n",
    "max_ = np.max([a['standard'] for a in ar])\n",
    "std_ = np.std([a['standard'] for a in ar])\n",
    "stats_items = [mean_,min_,max_]\n",
    "\n",
    "g = sns.distplot(standard_sci, label=\"Readability Index\")\n",
    "\n",
    "\n",
    "histogram_content = [x[0] for x in xys]\n",
    "height_content = np.array([x[1] for x in xys])\n",
    "\n",
    "hc = np.array(histogram_content)\n",
    "\n",
    "x_sub_set=histogram_content\n",
    "\n",
    "\n",
    "other_name=str('Phytochromobilin C15-Z,syn - C15-E,anti isomerization: concerted or stepwise?')\n",
    "worst_height,_ = get_heights([max_ART],hc,x_sub_set)\n",
    "mwp_height,_ = get_heights([mwp],hc,x_sub_set)\n",
    "\n",
    "#bmark_max_art\n",
    "worst_height = worst_height[0]\n",
    "#bmark_stats_items_grid = snap_to_grid(bmark_stats_items,bin_centers)\n",
    "\n",
    "#worst_distamnce = snap_to_grid(max_ART,bin_centers)\n",
    "worst_distance = snap_to_grid([max_ART],bin_centers)\n",
    "mwp_distance = snap_to_grid([mwp],bin_centers)\n",
    "x,y,z = (mwp_distance[0],mwp_height[0],str('mean wikipedia'))\n",
    "\n",
    "#print(bmark)\n",
    "bmark_stats_items = list(set([ b['standard'] for b in bmark ]))\n",
    "bmark_stats_items.append(x)\n",
    "#bmark_stats_items.append(max_ART)\n",
    "bmark_heights, _ = get_heights(bmark_stats_items,histogram_content,x_sub_set)\n",
    "heights, bwo = get_heights(stats_items,histogram_content,x_sub_set)\n",
    "#bmark_heights.append(worst_height)\n",
    "bmark_stats_items = [i+bwo for i in bmark_stats_items]\n",
    "mean_a = mean_\n",
    "min_a = min_ \n",
    "max_a = max_ \n",
    "xticks = list(range(0,45,5))\n",
    "\n",
    "#print(xticks)\n",
    "bmark_stats_items\n",
    "box_content = [a['standard'] for a in ar]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bmark_stats_items_grid = snap_to_grid(bmark_stats_items,bin_centers)\n",
    "author_stats =[i for i in [mean_,min_,max_]]\n",
    "author_stats_grid = snap_to_grid(author_stats,bin_centers)\n",
    "mean_a_grid = snap_to_grid([mean_a],bin_centers)\n",
    "x_sub_set_grid = snap_to_grid(x_sub_set,bin_centers)\n",
    "\n",
    "print(bmark_stats_items_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ \"Readibility of Science Declining Over Time\", \"Post Modern Essay Generator\",\"upgoer 5\",\"Science of Writing\",\"Mean Wikipedia\"]#\"Mean Wikipedia\"]#,other_name]# \"wikipedia science\"]\n",
    "bmark_stats_items_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalibrate_heights,b = get_heights(author_stats_grid,hc,x_sub_set)\n",
    "\n",
    "heights[0] = np.max(recalibrate_heights)\n",
    "heights[2] = recalibrate_heights[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(height_content)\n",
    "heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmark_heights\n",
    "print(len(bmark_heights))\n",
    "print(len(bin_centers))\n",
    "print(len(bmark_stats_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bmark_stats_items_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = np.array(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmark_stats_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories\n",
    "#categories.insert(3,'Mean Wikipedia Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xinterval\n",
    "x1,y1,z1 = (mwp_distance[0],mwp_height[0],str('mean wikipedia'))\n",
    "x1\n",
    "#bmark_heights[3]=y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(bmark_stats_items_grid)\n",
    "import copy\n",
    "#del bmark_stats_items_grid[-2]\n",
    "#del bmark_stats_items_grid[-1]\n",
    "xinterval1 = copy.copy(bmark_stats_items_grid)\n",
    "#xinterval1.insert(3,x1)\n",
    "#xinterval1\n",
    "#del bmark_heights[-1]\n",
    "bmark_heights\n",
    "print(len(bmark_heights))\n",
    "print(len(bmark_stats_items_grid))\n",
    "\n",
    "benchmarks = pd.DataFrame({\n",
    "'benchmarks': bmark_stats_items_grid,\n",
    "    'CDF': bmark_heights\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10, 10),nrows=2, ncols=1, sharex=True, dpi=100)\n",
    "\n",
    "\n",
    "g = sns.distplot(standard_sci, label=\"Readability Index\")\n",
    "\n",
    "\n",
    "if str('data0') not in locals():\n",
    "    data0 = pd.DataFrame({\n",
    "    'mean, min, maximum': author_stats_grid,\n",
    "        'CDF': heights\n",
    "        })\n",
    "\n",
    "\n",
    "    data2 = pd.DataFrame({\n",
    "    'Standard Reading Level': mean_a_grid,\n",
    "        'CDF': np.max(height_content)\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "legend_properties = {'weight':'bold','size':8}\n",
    "ax = sns.regplot(data=benchmarks, x=\"benchmarks\", y=\"CDF\", fit_reg=False, marker=\"o\", color=\"green\")\n",
    "ax = sns.regplot(data=data2, x=\"Standard Reading Level\", y=\"CDF\", fit_reg=False, marker=\"o\", color=\"red\")\n",
    "legendMain=ax.legend(labels=[str(\"std deviation\")], prop=legend_properties,loc='upper right')\n",
    "legendSide0=ax.legend(labels=[NAME],prop=legend_properties,loc='center right')\n",
    "legendSide1=ax.legend(labels=[str('Number of Documents: '+str(len(ar)))],prop=legend_properties,loc='upper left')\n",
    "legendMain=ax.legend(labels=[str(\"Google scholar author relative to ART Corpus distribution. Total docs: \")+str(len(trainingDats))], prop=legend_properties,loc='upper left')\n",
    "#\n",
    "print(categories)\n",
    "x,y,z = (worst_distance[0],worst_height,other_name)\n",
    "data3 = pd.DataFrame({\n",
    "'Standard Reading Level': [x1],\n",
    "    'CDF': [y1]\n",
    "    })\n",
    "ax = sns.regplot(data=data3, x='Standard Reading Level', y=\"CDF\", fit_reg=False, marker=\"o\", color=\"green\")\n",
    "\n",
    "\n",
    "axes[1] = ax = sns.regplot(data=benchmarks, x=\"benchmarks\", y=\"CDF\", fit_reg=False, marker=\"o\", color=\"green\")\n",
    "\n",
    "ax2 = plt.twiny()\n",
    "xticks = list(range(0,45,5))\n",
    "ax2.set_xticks(xticks)\n",
    "\n",
    "axes[1].set_xticks(xinterval1)\n",
    "axes[1].set_xticklabels(categories, minor=False, rotation=90)\n",
    "\n",
    "axes[1].axvline(np.mean(standard_sci), color='red', alpha=.7, linewidth=1.5)\n",
    "axes[1].set_ylabel('Probability of Document Reading Level')\n",
    "axes[1].set_xlabel('Reading Grade Level')\n",
    "bp_dict = axes[0].boxplot(box_content, 0, 'gD', vert=False)\n",
    "\n",
    "\n",
    "for line in bp_dict['medians']:\n",
    "    x, y = line.get_xydata()[1] # top of median line\n",
    "\n",
    "for line in bp_dict['boxes']:\n",
    "    x0, y = line.get_xydata()[0] # bottom of left line\n",
    "    axes[0].text(x0,y, str(NAME)+' Q1 ',horizontalalignment='center',verticalalignment='top',rotation=90)\n",
    "\n",
    "    x1, y = line.get_xydata()[3] # bottom of right line\n",
    "    axes[0]. text(x1,y, str(NAME)+' Q3 ',horizontalalignment='center',verticalalignment='top',rotation=90)\n",
    "\n",
    "    axes[0]. text(np.abs(x1+x0)/2,y, str(NAME)+' $\\mu$ ',horizontalalignment='center',verticalalignment='top',rotation=90)\n",
    "    x2, y = line.get_xydata()[1] # bottom of right line\n",
    "axes[0].axvline(np.mean(standard_sci), color='red', alpha=.7, linewidth=1.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmark_stats_items_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
